"""
Paginaci√≥n tradicional usando enlaces HTML.
"""

import logging
from typing import List, Dict, Any
from crawl4ai import AsyncWebCrawler

from crawler.pagination.base_pagination import BasePagination
# Importar funci√≥n legacy desde el m√≥dulo ra√≠z
import sys
import os
parent_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

try:
    from crawler.pagination import find_pagination_links
except ImportError:
    # Fallback si no se puede importar
    def find_pagination_links(html: str, base_url: str):
        return []

logger = logging.getLogger(__name__)


class GenericPagination(BasePagination):
    """
    Implementaci√≥n de paginaci√≥n tradicional (enlaces HTML).
    
    Busca enlaces de paginaci√≥n en el HTML y scrapea cada p√°gina individualmente.
    """
    
    async def scrape_pages(
        self,
        url: str,
        max_pages: int,
        crawler: AsyncWebCrawler,
        config: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Scrapea p√°ginas usando enlaces de paginaci√≥n tradicional.
        
        Args:
            url: URL inicial
            max_pages: N√∫mero m√°ximo de p√°ginas
            crawler: Instancia de AsyncWebCrawler
            config: Configuraci√≥n de Crawl4AI
            
        Returns:
            Lista de resultados (una entrada por p√°gina)
        """
        all_results = []
        
        # Scrapear primera p√°gina
        logger.info(f"üìÑ Procesando p√°gina 1 de {max_pages} para {url}")
        first_result = await crawler.arun(url=url)
        
        if not first_result.success:
            logger.warning(f"‚ö†Ô∏è Error al procesar p√°gina 1: {first_result.error_message}")
            return all_results
        
        # Procesar primera p√°gina
        first_page_result = {
            "success": True,
            "markdown": first_result.markdown.raw_markdown if first_result.markdown else "",
            "html": first_result.html or "",
            "url": url,
            "html_length": len(first_result.html or ""),
            "markdown_length": len(first_result.markdown.raw_markdown if first_result.markdown else "")
        }
        all_results.append(first_page_result)
        logger.info(f"‚úÖ P√°gina 1 procesada correctamente")
        
        # Buscar enlaces de paginaci√≥n
        html = first_result.html or ""
        pagination_links = find_pagination_links(html, url)
        
        # Limitar n√∫mero de p√°ginas
        pages_to_scrape = min(len(pagination_links), max_pages - 1)
        
        # Scrapear p√°ginas adicionales
        for i, page_url in enumerate(pagination_links[:pages_to_scrape], start=2):
            logger.info(f"üìÑ Procesando p√°gina {i} de {max_pages} para {page_url}")
            page_result = await crawler.arun(url=page_url)
            
            if page_result.success:
                page_data = {
                    "success": True,
                    "markdown": page_result.markdown.raw_markdown if page_result.markdown else "",
                    "html": page_result.html or "",
                    "url": page_url,
                    "html_length": len(page_result.html or ""),
                    "markdown_length": len(page_result.markdown.raw_markdown if page_result.markdown else "")
                }
                all_results.append(page_data)
                logger.info(f"‚úÖ P√°gina {i} procesada correctamente")
            else:
                logger.warning(f"‚ö†Ô∏è Error al procesar p√°gina {i}: {page_result.error_message}")
        
        return all_results

